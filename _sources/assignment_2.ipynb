{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (Evaluating RBF Kernels)\n",
    "\n",
    "In the module unit on CUDA we discussed the following code to evaluate radial basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def rbf_evaluation_cuda(sources, targets, weights, result):\n",
    "    local_result = cuda.shared.array((SX, nsources), numba.float32)\n",
    "    local_targets = cuda.shared.array((SX, 3), numba.float32)\n",
    "    local_sources = cuda.shared.array((SY, 3), numba.float32)\n",
    "    local_weights = cuda.shared.array(SY, numba.float32)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    px, py = cuda.grid(2)\n",
    "    \n",
    "    if px >= targets.shape[0]:\n",
    "        return\n",
    "\n",
    "    # At first we are loading all the targets into the shared memory\n",
    "    # We use only the first column of threads to do this.\n",
    "    \n",
    "    if ty == 0:\n",
    "        for index in range(3):\n",
    "            local_targets[tx, index] = targets[px, index]\n",
    "    \n",
    "    # We are now loading all the sources and weights.\n",
    "    # We only require the first row of threads to do this.\n",
    "    \n",
    "    if tx == 0:\n",
    "        for index in range(3):\n",
    "            local_sources[ty, index] = sources[py, index]\n",
    "        local_weights[ty] = weights[ty]\n",
    "        \n",
    "    # Let us now sync all threads\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Now compute the interactions\n",
    "    \n",
    "    squared_diff = numba.float32(0)\n",
    "    \n",
    "    for index in range(3):\n",
    "        squared_diff += (local_targets[tx, index] - local_sources[ty, index])**2\n",
    "    local_result[tx, ty] = math.exp(-squared_diff / ( numba.float32(2) * numba.float32(sigma)**2)) * local_weights[ty]\n",
    "    \n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Now sum up all the local results\n",
    "    \n",
    "    if ty == 0:\n",
    "        res = numba.float32(0)\n",
    "        for index in range(nsources):\n",
    "            res += local_result[tx, index]\n",
    "        result[px] = res    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a threadblock layout so that each threadblock evaluated all sources with a given number of target points. This works well for small numbers of source points, but not for larger numbers of sources. Your task is to extend this code to deal with larger numbers of sources.\n",
    "\n",
    "The idea is the following. Create threadblocks of size 16 x 32 which are responsible for 16 targets and 32 sources. Each threadblock now evaluates its own local result, which is an array of size 16 and writes the result back into global memory. You then create a second kernel that runs over the intermediate results and sums up the intermediate results into the final sum.\n",
    "\n",
    "Let's make this precise. We assume that we have m targets and n sources. We therefore need to create a grid of $(\\ell, p)$ thread blocks with $\\ell = (m+15) / 16$ and $p = (n + 31) / 32$, where the division is to be understood as integer division. \n",
    "\n",
    "* Use the CUDA memory transfer functions to copy the sources and targets to the compute device.\n",
    "* On the compute device create an array for the intermediate results that is of size (m, p).\n",
    "* Launch threadblocks that evaluate the partial sum for 16 targets and 32 sources at a time and store the results in the corresponding part of the (m, p) array.\n",
    "* Then launch a summation kernel of m threads, where each thread sums up the p numbers in its row of the intermediate array (m, p) and store into a result array of dimension m.\n",
    "* Finally, copy the end result back to the CPU and return to the user.\n",
    "\n",
    "Be careful with memory transfers. We do not want to transfer the intermediate array back and forth between CPU and GPU. So you need to manually create the device arrays and launch the CUDA kernels with the device arrays.\n",
    "\n",
    "Demonstrate that your code is correct by validating it with the Python Numba implementation that we have written and show that up to single precision accuracy your result agrees with this implementation. **You will lose significant marks if you do not validate your code as without validation the correctness is not demonstrated.**\n",
    "\n",
    "Show benchmark results and experiment with how many sources and targets you can evaluate in a reasonable time (around a few seconds of runtime). Also, separately demonstrate benchmark times for the memory transfers from the CPU to the GPU and back, and benchmarks for the actual computation on the device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (Evaluating the discrete Laplacian on GPUs)\n",
    "\n",
    "In the Lectures we have discussed the function `discretise_poisson` that generates a sparse matrix which is the discretisation of the Laplace operator with zero boundary conditions.\n",
    "In this exercise you are asked to create a GPU kernel, which given a one-dimensional array of values $u_{i, j}$ in the unit square grid (see the corresponding notebook on the discrete Laplace problem), evaluates this discrete Laplace operator without explicity creatiing a matrix.\n",
    "\n",
    "The idea is as follows.\n",
    "\n",
    "Create $N^2$ threads. Each thread checks if its associated with a boundary value or an interior value. If it is associated with a boundary value, just write the corresponding $u_{i, j}$ value in the result array (because our sparse matrix was just the identity for boundary values). If it is associated with an interior point, write the value of evaluating the 5 point stencil for the corresponding interior point into the result array.\n",
    "\n",
    "**Validate your code against the matrix vector multiplication with the matrix created in the `discrete_poisson` function. Your code must return the same result as it evaluates the same computation. Again, lack of validation will lead to substantial loss of marks.**\n",
    "\n",
    "Benchmark your code for growing matrix sizes n. What is the ratio of data movements to computations in your code?\n",
    "\n",
    "Describe how you could use shared memory to improve the memory behavior and reduce global memory accesses. You need not provide a code implementation of your idea. But try to be precise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment of the coursework\n",
    "\n",
    "* The submission deadline for the coursework is **Monday 9 November, 10am**.\n",
    "* Each part counts 50% of the total mark.\n",
    "* The assignment does not require much code writing. But a strong emphasis is put on good explanations. Putting a few comment lines in your code is not sufficient as explanation. Use the Jupyter notebook capabilities and write good explanations about what you are doing and why you are doing it as markdown cells.\n",
    "* Your code must be executable without any errors from scratch in Jupyter by choosing \"Restart kernel and run all cells.\" If this produces any errors, any code and explanations after the error occurs will be ignored for the marking. It is not the task of the markers to debug your code.\n",
    "* The code should not run for much longer than 2 minutes on a typical laptop/desktop. This is a soft limit. If your notebook runs too long we reserve the right to reject it.\n",
    "* In addition to core Python packages you are allowed to use Numpy, Scipy. Numba, matplotlib and the timing functions from Jupyter/IPython. No other packages are allowed and any such request will be rejected.\n",
    "* You must submit your solution as a single Jupyter Notebook file with ending `*.ipynb`. **Any other submission will lead to 0 marks automatically. Make sure you submit a correct Notebook file with the right ending.**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
